{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import gzip\n",
        "import platform\n",
        "import pickle\n",
        "import struct"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:01.431Z",
          "iopub.status.busy": "2020-06-11T17:27:01.424Z",
          "iopub.status.idle": "2020-06-11T17:27:01.489Z",
          "shell.execute_reply": "2020-06-11T17:27:01.496Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>\n",
        "\n",
        "### Fully-connected layer"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fc_fp(z, W, b):\n",
        "    \"\"\" forward propagation of a fully-connected layer\n",
        "    :param z: output of the previous layer | shape(n,l1)\n",
        "    :param W: weight of this layer | shape(l1,l2)\n",
        "    :param b: bias of this layer | shape(l2,)\n",
        "    :return z_fc: output of this layer | shape(n,l2)\n",
        "    \"\"\"\n",
        "    z_fc = np.dot(z, W) + b\n",
        "    return z_fc\n",
        "\n",
        "# # test\n",
        "# z = [[1,2,3],[1,2,3],[1,2,3]]\n",
        "# W = [[1,2], [3,4], [5,6]]\n",
        "# b = [10,20]\n",
        "# print(fc_fp(z, W, b))"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:02.497Z",
          "iopub.status.busy": "2020-06-11T17:27:02.491Z",
          "iopub.status.idle": "2020-06-11T17:27:02.508Z",
          "shell.execute_reply": "2020-06-11T17:27:02.514Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fc_bp(dz1, W, z):\n",
        "    \"\"\" backward propagation of a fully-connected layer\n",
        "    :param dz1: gradient of z in next layer | shape(n,l2)\n",
        "    :param W: weight of this layer | shape(l1,l2)\n",
        "    :param z: input of this layer | shape(n, l1)\n",
        "    :return: \n",
        "        dW: gradient of weight in this layer | shape(l1,l2)\n",
        "        db: gradient of bias in this layer | shape(l2,)\n",
        "        dz: gradient of values in this layer | shape(n,l2)\n",
        "    \"\"\"\n",
        "    n = z.shape[0]\n",
        "    dW = np.dot(z.T, dz1)/n\n",
        "    db = np.sum(dz1, axis=0)/n\n",
        "    dz = np.dot(dz1, W.T)\n",
        "    return dW, db, dz"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:02.921Z",
          "iopub.status.busy": "2020-06-11T17:27:02.914Z",
          "iopub.status.idle": "2020-06-11T17:27:02.934Z",
          "shell.execute_reply": "2020-06-11T17:27:02.942Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional layer"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2d_fp(z, K, b, padding=(0,0)):\n",
        "    \"\"\" forward propagation of a convolutional layer\n",
        "    :param z: output of the previous layer | shape(n,c1,h1,w1)\n",
        "    :param K: kernel of this layer | shape(c1,c2,k1,k2)\n",
        "    :param b: bias of this layer | shape(c2,)\n",
        "    :param padding: padding size | shape(p1,p2)\n",
        "    :return z_conv: output of this layer | shape(n,c2,h2,w2)\n",
        "    \"\"\"\n",
        "    # padding\n",
        "    p1 = padding[0]\n",
        "    p2 = padding[1]\n",
        "    # z_p(n,c1,h1p,w1p)\n",
        "    z_p = np.lib.pad(z, ((0,0),(0,0),(p1,p1),(p2,p2)), mode='constant', constant_values=0)\n",
        "    n, c1, h1, w1 = z_p.shape\n",
        "    c1, c2, k1, k2 = K.shape\n",
        "    # convolution\n",
        "    h2 = h1-k1+1\n",
        "    w2 = w1-k2+1\n",
        "    z_conv = np.zeros((n, c2, h2, w2))\n",
        "    for num in range(n):\n",
        "        for c in range(c2):\n",
        "            for h in range(h2):\n",
        "                for w in range(w2):\n",
        "                    # z_part(1,c1,kernel,kernel)\n",
        "                    # K_part(c1,1,kernel,kernel)\n",
        "                    # z_part * K_part(1,1,kernel,kernel)\n",
        "                    z_part = z_p[num, :, h:h+k1, w:w+k2]\n",
        "                    K_part = K[:,c]\n",
        "                    z_conv[num, c, h, w] = np.sum(z_part * K_part) + b[c]\n",
        "    return z_conv"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:03.586Z",
          "iopub.status.busy": "2020-06-11T17:27:03.578Z",
          "iopub.status.idle": "2020-06-11T17:27:03.594Z",
          "shell.execute_reply": "2020-06-11T17:27:03.602Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2d_bp(dz1, K, z, padding=(0,0)):\n",
        "    \"\"\" backward propagation of a 2D-convolutional layer\n",
        "    :param dz1: gradient of values in the next layer | shape(n,c2,h2,w2)\n",
        "    :param K: kernel of this layer | shape(c1,c2,k1,k2)\n",
        "    :param z: values of this layer | shape(n,c1,h1,w1)\n",
        "    :param padding: padding size | shape(p1,p2)\n",
        "    :return: \n",
        "        dK: gradient of kernel in this layer | shape(c1,c2,k1,k2)\n",
        "        db: gradient of bias in this layer | shape(c2,)\n",
        "        dz: gradient of values in this layer | shape(n,c1,h1,w1)\n",
        "    \"\"\"\n",
        "    c1, c2, k1, k2 = K.shape\n",
        "    n = z.shape[0]\n",
        "    p1 = padding[0]\n",
        "    p2 = padding[1]\n",
        "    \n",
        "    # z(n,c1,h1,w1) -> z_sw(c1,n,h1,w1) -> z_sw_p(c1,n,h1p,w1p)\n",
        "    # dz1(n,c2,h2,w2) = (n, c2, h1p-kernel, h2p-kernel)\n",
        "    # dK: shape(c1,c2,k1,k2)\n",
        "    z_sw = np.swapaxes(z, 0, 1)\n",
        "    dK = conv2d_fp(z_sw, dz1, np.zeros((c2,)), (p1, p2))/n\n",
        "    \n",
        "    # db: shape(c2,)\n",
        "    db = np.sum(dz1, axis=(0, 2, 3))/n\n",
        "    \n",
        "    # dz1 padding: shape(n, c2, h2+2kernel, w2+2kernel) = (n, c2, h1p+kernel, w1p+kernel)\n",
        "    # modify K: shape(c1,c2,k1,k2) -> rotation(c1,c2,k1,k2) -> Kr(c2,c1,k1,k2)\n",
        "    # padded dz: shape(n,c1,h1p,w1p)\n",
        "    dz1_p = np.lib.pad(dz1, ((0,0), (0,0), (k1-1, k1-1), (k2-1, k2-1)), mode='constant', constant_values=0)\n",
        "    Kr = np.swapaxes(np.flip(K, (2,3)), 0,1)\n",
        "    dz_p = conv2d_fp(dz1_p, Kr, np.zeros((c2,)))\n",
        "    # remove padding of dz_p(n,c1,h1p,w1p) -> dz(n,c1,h1,w1)\n",
        "    dz = dz_p\n",
        "    if(p1 > 0 and p2 > 0):\n",
        "        dz = dz[:, :, p1:-p1, p2:-p2]\n",
        "    elif p1 > 0:\n",
        "        dz = dz[:, :, p1:-p1, :]\n",
        "    elif p2 > 0:\n",
        "        dz = dz[:, :, :, p2:-p2]\n",
        "    return dK, db, dz"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:03.786Z",
          "iopub.status.busy": "2020-06-11T17:27:03.780Z",
          "iopub.status.idle": "2020-06-11T17:27:03.795Z",
          "shell.execute_reply": "2020-06-11T17:27:03.804Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>\n",
        "\n",
        "### Pooling layer - Max pooling"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maxpool_fp(z, pool_size=(2,2), pool_stride=(2,2), padding=(0,0)):\n",
        "    \"\"\" forward propagation of a max-pooling layer\n",
        "    :param z: output of the previous layer | shape(n,c1,h1,w1)\n",
        "    :param pool_size: size of pooling kernel | shape(k1,k2)\n",
        "    :param pool_stride: stride of pooling kernel | shape(s1,s2)\n",
        "    :param padding: padding size | shape(p1,p2)\n",
        "    :return z_po: output of this layer | shape(n,c1,h2,w2)\n",
        "    \"\"\"\n",
        "    n, c1, h1, w1 = z.shape\n",
        "    k1 = pool_size[0]\n",
        "    k2 = pool_size[1]\n",
        "    s1 = pool_stride[0]\n",
        "    s2 = pool_stride[1]\n",
        "    p1 = padding[0]\n",
        "    p2 = padding[1]\n",
        "    h2 = (h1 + p1*2 - k1)//s1 + 1\n",
        "    w2 = (w1 + p2*2 - k2)//s2 + 1\n",
        "    # padding of z\n",
        "    z_p = np.lib.pad(z, ((0,0), (0,0), (p1, p1), (p2, p2)), 'constant', constant_values=0)\n",
        "    z_po = np.zeros((n,c1,h2,w2))\n",
        "    for num in range(n):\n",
        "        for c in range(c1):\n",
        "            for h in range(h2):\n",
        "                for w in range(w2):\n",
        "                    z_part = z_p[num, c, h*s1:h*s1+k1, w*s2:w*s2+k2]\n",
        "                    z_po[num,c,h,w] = np.max(z_part)\n",
        "    return z_po\n",
        "  \n",
        "# # test\n",
        "# z = np.random.randn(10, 2, 6, 6)\n",
        "# z_po = maxpool_fp(z, padding=(3,3))\n",
        "# print(z.shape)\n",
        "# print(z_po.shape)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:04.515Z",
          "iopub.status.busy": "2020-06-11T17:27:04.509Z",
          "iopub.status.idle": "2020-06-11T17:27:04.524Z",
          "shell.execute_reply": "2020-06-11T17:27:04.531Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maxpool_bp(dz1, z, pool_size=(2,2), pool_stride=(2,2), padding=(0,0)):\n",
        "    \"\"\" backward propagation of a max-pooling layer\n",
        "    :param dz1: gradient of values in the next layer | shape(n,c1,h2,w2)\n",
        "    :param z: values of this layer | shape(n,c1,h1,w1)\n",
        "    :param pool_size: size of pooling kernel | shape(k1,k2)\n",
        "    :param pool_stride: stride of pooling kernel | shape(s1,s2)\n",
        "    :param padding: padding size | shape(p1,p2)\n",
        "    :return dz: gradient of values in this layer | shape(n,c1,h1,w1)\n",
        "    \"\"\"\n",
        "    n, c1, h1, w1 = z.shape\n",
        "    n, c2, h2, w2 = dz1.shape\n",
        "    k1 = pool_size[0]\n",
        "    k2 = pool_size[1]\n",
        "    s1 = pool_stride[0]\n",
        "    s2 = pool_stride[1]\n",
        "    p1 = padding[0]\n",
        "    p2 = padding[1]\n",
        "    # padding of z: shape(n,c1,h1p,w1p)\n",
        "    z_p = np.lib.pad(z, ((0,0), (0,0), (p1, p1), (p2, p2)), 'constant', constant_values=0)\n",
        "    # padded dz: shape(n,c1,h1p,w1p)\n",
        "    dz_p = np.zeros(z_p.shape)\n",
        "    for num in range(n):\n",
        "        for c in range(c1):\n",
        "            for h in range(h2):\n",
        "                for w in range(w2):\n",
        "                    # associated part in z_p of the element dz1[num, c, h, w]\n",
        "                    z_part = z_p[num, c, h*s1:h*s1+k1, w*s2:w*s2+k2]\n",
        "                    # index of max in z_part\n",
        "                    id_max_z_part = np.argmax(z_part)\n",
        "                    # index of this max in z_p\n",
        "                    h_max_z_p = h*s1 + id_max_z_part // k2\n",
        "                    w_max_z_p = w*s2 + id_max_z_part % k2\n",
        "                    dz_p[num, c, h_max_z_p, w_max_z_p] += dz1[num, c, h, w]\n",
        "    # remove padding of dz_p(n,c1,h1p,w1p) -> dz(n,c1,h1,w1)\n",
        "    dz = dz_p\n",
        "    if(p1 > 0 and p2 > 0):\n",
        "        dz = dz[:, :, p1:-p1, p2:-p2]\n",
        "    elif p1 > 0:\n",
        "        dz = dz[:, :, p1:-p1, :]\n",
        "    elif p2 > 0:\n",
        "        dz = dz[:, :, :, p2:-p2]\n",
        "    return dz\n",
        "  \n",
        "# # test\n",
        "# z = np.array([[[[1,5,8,6],\n",
        "#        [2,4,6,4],\n",
        "#        [3,5,7,3],\n",
        "#        [5,10,3,5]]]])\n",
        "# z_po = maxpool_fp(z, pool_size=(2,2), pool_stride=(1,1), padding=(0,0))\n",
        "# dz1 = z_po/5\n",
        "# dz = maxpool_bp(dz1, z, pool_size=(2,2), pool_stride=(1,1), padding=(0,0))\n",
        "# print(z_po)\n",
        "# print(dz1)\n",
        "# print(dz)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:04.590Z",
          "iopub.status.busy": "2020-06-11T17:27:04.584Z",
          "iopub.status.idle": "2020-06-11T17:27:04.601Z",
          "shell.execute_reply": "2020-06-11T17:27:04.608Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>\n",
        "\n",
        "### Flatten layer"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_fp(z):\n",
        "    \"\"\" change the dimention of data: 2D to 1D\n",
        "    :param z: output of previous layer | shape(n,c1,h1,w1)\n",
        "    :return z_flt: output after flattening | shape(n,l1)\n",
        "    \"\"\"\n",
        "    n = z.shape[0]\n",
        "    z_flt = np.reshape(z, (n,-1))\n",
        "    return z_flt\n",
        "  \n",
        "# test\n",
        "# z = np.random.randn(2, 2, 3, 3)\n",
        "# z_flt = flatten_fp(z)\n",
        "# print(z.shape)\n",
        "# print(z_flt.shape)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:04.758Z",
          "iopub.status.busy": "2020-06-11T17:27:04.751Z",
          "iopub.status.idle": "2020-06-11T17:27:04.767Z",
          "shell.execute_reply": "2020-06-11T17:27:04.773Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_bp(dz1, z):\n",
        "    \"\"\" backward propagation of flatten layer\n",
        "    :param dz1: gradient of z in next layer | shape(n,l1)\n",
        "    :param z: output of previous layer | shape(n,c1,h1,w1)\n",
        "    :return dz: gradient of z in this layer | shape(n,c1,h1,w1)\n",
        "    \"\"\"\n",
        "    dz = np.reshape(dz1, z.shape)\n",
        "    return dz\n",
        "\n",
        "# # test\n",
        "# z = np.random.randn(2, 2, 3, 3)\n",
        "# dz1 = flatten_fp(z)\n",
        "# dz = flatten_bp(dz1, z)\n",
        "# print(z.shape)\n",
        "# print(dz1.shape)\n",
        "# print(dz.shape)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:04.836Z",
          "iopub.status.busy": "2020-06-11T17:27:04.822Z",
          "iopub.status.idle": "2020-06-11T17:27:04.845Z",
          "shell.execute_reply": "2020-06-11T17:27:04.850Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>\n",
        "\n",
        "### Activate function"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_fp(z):\n",
        "    \"\"\" forward propagation of activate function - Relu\n",
        "    :param z: output of previous layer | shape(n,l1)\n",
        "    :return: output after activate layer Relu | shape(n,l1)\n",
        "    \"\"\"\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "\n",
        "def relu_bp(dz1, z):\n",
        "    \"\"\" backward propagation of activate function - Relu\n",
        "    :param dz1: gradient of z in next layer | shape(n,l2=l1)\n",
        "    :param z: output of previous layer | shape(n,l1)\n",
        "    :return dz: gradient of z in this layer | shape(n,l1)\n",
        "    \"\"\"\n",
        "    dz = np.where(np.greater(z, 0), dz1, 0)\n",
        "    return dz"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:05.004Z",
          "iopub.status.busy": "2020-06-11T17:27:04.998Z",
          "iopub.status.idle": "2020-06-11T17:27:05.015Z",
          "shell.execute_reply": "2020-06-11T17:27:05.021Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh_fp(z):\n",
        "    \"\"\" forward propagation of activate function - tanh\n",
        "    :param z: output of previous layer\n",
        "    :return: output after activate layer Relu\n",
        "    \"\"\"\n",
        "    return np.tanh(z)\n",
        "  \n",
        "\n",
        "def tanh_bp(dz1):\n",
        "    \"\"\" backward propagation of activate function - tanh\n",
        "    :param dz1: gradient of z in next layer\n",
        "    :return dz: gradient of z in this layer\n",
        "    \"\"\"\n",
        "    dz = 1 - np.square(np.tanh(dz1))\n",
        "    return dz"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:05.069Z",
          "iopub.status.busy": "2020-06-11T17:27:05.062Z",
          "iopub.status.idle": "2020-06-11T17:27:05.080Z",
          "shell.execute_reply": "2020-06-11T17:27:05.086Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>\n",
        "\n",
        "### Loss function - mean squared loss, softmax & cross entropy"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared(y, yr):\n",
        "    \"\"\" loss function - mean squared loss\n",
        "    :param y: output of the last layer | shape(n,ln)\n",
        "    :param yr: real value of the samples | shape(n,ln)\n",
        "    :return:\n",
        "        loss: total loss | shape(1,)\n",
        "        dy: gradient of values in this layer | shape(n,ln)\n",
        "    \"\"\"\n",
        "    sq = np.square(y - yr)/2\n",
        "    loss = np.mean(np.sum(sq, axis=-1))\n",
        "    dy = y - yr\n",
        "    return loss, dy\n",
        "  \n",
        "# #test \n",
        "# y = np.array([[0.1,0.1,0.8],[0.1,0.1,0.8],[0.2,0.2,0.6],[0.01,0.01,0.98]])\n",
        "# yr = np.array([[0,0,1],[0,0,1],[0,0,1],[0,0,1]])\n",
        "# loss, dy = mean_squared(y, yr)\n",
        "# print(loss)\n",
        "# print(dy)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:05.250Z",
          "iopub.status.busy": "2020-06-11T17:27:05.244Z",
          "iopub.status.idle": "2020-06-11T17:27:05.259Z",
          "shell.execute_reply": "2020-06-11T17:27:05.264Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(y):\n",
        "    \"\"\" softmax function\n",
        "    :param y: output of the last layer | shape(n,ln)\n",
        "    :return: y after softmax | shape(n,ln)\n",
        "    \"\"\"\n",
        "    return (np.exp(y).T/np.sum(np.exp(y), axis=-1)).T\n",
        "  \n",
        "# # test\n",
        "# y = np.array([[1,2,3],[1,2,3],[1,2,5]])\n",
        "# res = softmax(y)\n",
        "# print(res)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:05.734Z",
          "iopub.status.busy": "2020-06-11T17:27:05.728Z",
          "iopub.status.idle": "2020-06-11T17:27:05.742Z",
          "shell.execute_reply": "2020-06-11T17:27:05.747Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(y, yr):\n",
        "    \"\"\" loss function - cross entropy\n",
        "    :param y: output of the last layer | shape(n,ln)\n",
        "    :param yr: real value of the samples | shape(n,ln)\n",
        "    :return: \n",
        "        loss: total loss | shape(1,)\n",
        "        dy: gradient of values in this layer | shape(n,ln)\n",
        "    \"\"\"\n",
        "    sftmx = softmax(y)\n",
        "    lg = -np.log(sftmx)\n",
        "    yr_log = yr * lg\n",
        "    loss = np.mean(np.sum(yr_log, axis=-1))\n",
        "    dy = sftmx - yr\n",
        "    return loss, dy\n",
        "    \n",
        "# #test \n",
        "# y = np.array([[0.1,0.1,0.8],[0.1,0.1,0.8],[0.2,0.2,0.6],[0.01,0.01,0.98]])\n",
        "# yr = np.array([[0,0,1],[0,0,1],[0,0,1],[0,0,1]])\n",
        "# loss, dy = corss_entropy(y, yr)\n",
        "# print(loss)\n",
        "# print(dy)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:06.035Z",
          "iopub.status.busy": "2020-06-11T17:27:06.029Z",
          "iopub.status.idle": "2020-06-11T17:27:06.043Z",
          "shell.execute_reply": "2020-06-11T17:27:06.050Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>\n",
        "\n",
        "### Optimizer"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(weights, gradients, lr=0.01):\n",
        "    \"\"\" use SGD to optimize parameters in the network\n",
        "    :param weights: current parameters | dictionnary\n",
        "    :param gradients: current gradients | dictionnary\n",
        "    :param lr: learning rate\n",
        "    \"\"\"\n",
        "    for itm in weights.keys():\n",
        "        weights[itm] = weights[itm] - lr*gradients[itm]"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:07.140Z",
          "iopub.status.busy": "2020-06-11T17:27:07.134Z",
          "iopub.status.idle": "2020-06-11T17:27:07.150Z",
          "shell.execute_reply": "2020-06-11T17:27:07.156Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_md(weights, gradients, v_weights, lr=0.01, it=0, momentum=0.9, decay=1e-5):\n",
        "    \"\"\" use SGD with momentum and decay to optimize parameters in the network\n",
        "    :param weights: current parameters | dictionnary\n",
        "    :param gradients: current gradients | dictionnary\n",
        "    :param v_weights: amount of update of weights | dictionnary\n",
        "    :param lr: learning rate\n",
        "    :param momentum: momentum of parameters\n",
        "    :param decay: decay of learning rate\n",
        "    :return:\n",
        "        lr: updated learning rate\n",
        "        it: number of iteration\n",
        "    \"\"\"\n",
        "    # update leaning rate\n",
        "    lr = lr / (1 + decay*it)\n",
        "    # upgrade weights\n",
        "    for itm in weights.keys():\n",
        "        v_weights[itm] = momentum * v_weights[itm] + lr*gradients[itm]\n",
        "        weights[itm] -= v_weights[itm]\n",
        "    it += 1\n",
        "    return lr, it"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:07.397Z",
          "iopub.status.busy": "2020-06-11T17:27:07.390Z",
          "iopub.status.idle": "2020-06-11T17:27:07.406Z",
          "shell.execute_reply": "2020-06-11T17:27:07.412Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the file of train images\n",
        "train_images_idx3_ubyte_file = 'train-images.idx3-ubyte'\n",
        "# the file of train labels\n",
        "train_labels_idx1_ubyte_file = 'train-labels.idx1-ubyte'\n",
        "\n",
        "# the file of test images\n",
        "test_images_idx3_ubyte_file = 't10k-images.idx3-ubyte'\n",
        "# the file of test labels\n",
        "test_labels_idx1_ubyte_file = 't10k-labels.idx1-ubyte'\n",
        "\n",
        "\n",
        "def decode_idx3_ubyte(idx3_ubyte_file):\n",
        "    \"\"\"\n",
        "    decode ''.idx3-ubyte' file\n",
        "    :param idx3_ubyte_file: the path of '.idx3-ubyte' file\n",
        "    :return: images | shape(n, rows, cols)\n",
        "    \"\"\"\n",
        "    bin_data = open(idx3_ubyte_file, 'rb').read()\n",
        "\n",
        "    # decode file header information\n",
        "    offset = 0\n",
        "    fmt_header = '>iiii'\n",
        "    magic_number, num_images, num_rows, num_cols = struct.unpack_from(fmt_header, bin_data, offset)\n",
        "    print ('magic number:%d, images number: %d, image size: %d*%d' % (magic_number, num_images, num_rows, num_cols))\n",
        "\n",
        "    # decode data set\n",
        "    image_size = num_rows * num_cols\n",
        "    offset += struct.calcsize(fmt_header)\n",
        "    fmt_image = '>' + str(image_size) + 'B'\n",
        "    images = np.empty((num_images, num_rows, num_cols))\n",
        "    for i in range(num_images):\n",
        "        if (i + 1) % 10000 == 0:\n",
        "            print ('have decoded %d' % (i + 1) + 'images')\n",
        "        images[i] = np.array(struct.unpack_from(fmt_image, bin_data, offset)).reshape((num_rows, num_cols))\n",
        "        offset += struct.calcsize(fmt_image)\n",
        "    return images\n",
        "\n",
        "\n",
        "def decode_idx1_ubyte(idx1_ubyte_file):\n",
        "    \"\"\"\n",
        "    decode ''.idx1-ubyte' file\n",
        "    :param idx1_ubyte_file: the path of '.idx1-ubyte' file\n",
        "    :return: labels | shape(n, 1)\n",
        "    \"\"\"\n",
        "    bin_data = open(idx1_ubyte_file, 'rb').read()\n",
        "\n",
        "    # decode file header information\n",
        "    offset = 0\n",
        "    fmt_header = '>ii'\n",
        "    magic_number, num_images = struct.unpack_from(fmt_header, bin_data, offset)\n",
        "    print ('magic number:%d, images number: %d' % (magic_number, num_images))\n",
        "\n",
        "    # decode data set\n",
        "    offset += struct.calcsize(fmt_header)\n",
        "    fmt_image = '>B'\n",
        "    labels = np.empty(num_images)\n",
        "    for i in range(num_images):\n",
        "        if (i + 1) % 10000 == 0:\n",
        "            print ('have decoded %d' % (i + 1) + 'images')\n",
        "        labels[i] = struct.unpack_from(fmt_image, bin_data, offset)[0]\n",
        "        offset += struct.calcsize(fmt_image)\n",
        "    return labels\n",
        "\n",
        "\n",
        "def load_train_images(idx_ubyte_file=train_images_idx3_ubyte_file):\n",
        "    \"\"\"\n",
        "    TRAINING SET IMAGE FILE (train-images-idx3-ubyte):\n",
        "    [offset] [type]          [value]          [description]\n",
        "    0000     32 bit integer  0x00000803(2051) magic number\n",
        "    0004     32 bit integer  60000            number of images\n",
        "    0008     32 bit integer  28               number of rows\n",
        "    0012     32 bit integer  28               number of columns\n",
        "    0016     unsigned byte   ??               pixel\n",
        "    0017     unsigned byte   ??               pixel\n",
        "    ........\n",
        "    xxxx     unsigned byte   ??               pixel\n",
        "    Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).\n",
        "\n",
        "    :param idx_ubyte_file: the path of train images\n",
        "    :return: images | shape(n,28,28)\n",
        "    \"\"\"\n",
        "    return decode_idx3_ubyte(idx_ubyte_file)\n",
        "\n",
        "\n",
        "def load_train_labels(idx_ubyte_file=train_labels_idx1_ubyte_file):\n",
        "    \"\"\"\n",
        "    TRAINING SET LABEL FILE (train-labels-idx1-ubyte):\n",
        "    [offset] [type]          [value]          [description]\n",
        "    0000     32 bit integer  0x00000801(2049) magic number (MSB first)\n",
        "    0004     32 bit integer  60000            number of items\n",
        "    0008     unsigned byte   ??               label\n",
        "    0009     unsigned byte   ??               label\n",
        "    ........\n",
        "    xxxx     unsigned byte   ??               label\n",
        "    The labels values are 0 to 9.\n",
        "\n",
        "    :param idx_ubyte_file: the path of train labels\n",
        "    :return: labels | shape(n,1)\n",
        "    \"\"\"\n",
        "    return decode_idx1_ubyte(idx_ubyte_file)\n",
        "\n",
        "\n",
        "def load_test_images(idx_ubyte_file=test_images_idx3_ubyte_file):\n",
        "    \"\"\"\n",
        "    TEST SET IMAGE FILE (t10k-images-idx3-ubyte):\n",
        "    [offset] [type]          [value]          [description]\n",
        "    0000     32 bit integer  0x00000803(2051) magic number\n",
        "    0004     32 bit integer  10000            number of images\n",
        "    0008     32 bit integer  28               number of rows\n",
        "    0012     32 bit integer  28               number of columns\n",
        "    0016     unsigned byte   ??               pixel\n",
        "    0017     unsigned byte   ??               pixel\n",
        "    ........\n",
        "    xxxx     unsigned byte   ??               pixel\n",
        "    Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).\n",
        "\n",
        "    :param idx_ubyte_file: the path of test images\n",
        "    :return: images | shape(n,28,28)\n",
        "    \"\"\"\n",
        "    return decode_idx3_ubyte(idx_ubyte_file)\n",
        "\n",
        "\n",
        "def load_test_labels(idx_ubyte_file=test_labels_idx1_ubyte_file):\n",
        "    \"\"\"\n",
        "    TEST SET LABEL FILE (t10k-labels-idx1-ubyte):\n",
        "    [offset] [type]          [value]          [description]\n",
        "    0000     32 bit integer  0x00000801(2049) magic number (MSB first)\n",
        "    0004     32 bit integer  10000            number of items\n",
        "    0008     unsigned byte   ??               label\n",
        "    0009     unsigned byte   ??               label\n",
        "    ........\n",
        "    xxxx     unsigned byte   ??               label\n",
        "    The labels values are 0 to 9.\n",
        "\n",
        "    :param idx_ubyte_file: the path of test labels\n",
        "    :return: labels | shape(n,1)\n",
        "    \"\"\"\n",
        "    return decode_idx1_ubyte(idx_ubyte_file)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:08.226Z",
          "iopub.status.busy": "2020-06-11T17:27:08.219Z",
          "iopub.status.idle": "2020-06-11T17:27:08.234Z",
          "shell.execute_reply": "2020-06-11T17:27:08.240Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = load_train_images()\n",
        "train_labels = load_train_labels()\n",
        "test_images = load_test_images()\n",
        "test_labels = load_test_labels()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "magic number:2051, images number: 60000, image size: 28*28\n",
            "have decoded 10000images\n",
            "have decoded 20000images\n",
            "have decoded 30000images\n",
            "have decoded 40000images\n",
            "have decoded 50000images\n",
            "have decoded 60000images\n",
            "magic number:2049, images number: 60000\n",
            "have decoded 10000images\n",
            "have decoded 20000images\n",
            "have decoded 30000images\n",
            "have decoded 40000images\n",
            "have decoded 50000images\n",
            "have decoded 60000images\n",
            "magic number:2051, images number: 10000, image size: 28*28\n",
            "have decoded 10000images\n",
            "magic number:2049, images number: 10000\n",
            "have decoded 10000images\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:09.102Z",
          "iopub.status.busy": "2020-06-11T17:27:09.095Z",
          "iopub.status.idle": "2020-06-11T17:27:12.915Z",
          "shell.execute_reply": "2020-06-11T17:27:12.937Z"
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Preprocess data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(y, num_classes=None):\n",
        "    \"\"\"\n",
        "    Converts a class vector (integers) to binary class matrix (one-hot encoding).\n",
        "    :param y: class vector to be converted into a matrix (integers from 0 to num_classes). | shape(n,1)\n",
        "    :param num_classes: total number of classes.\n",
        "    :return categorical: A binary matrix representation of the input. | shape(n,num_classes)\n",
        "    \"\"\"\n",
        "    if num_classes==None:\n",
        "        num_classes = np.max(y)+1\n",
        "    # the number of samples\n",
        "    n = y.shape[0]\n",
        "    categorical = np.zeros((n,num_classes), dtype=np.float32)\n",
        "    for i in range(n):\n",
        "        categorical[i][int(y[i])] = 1\n",
        "    return categorical"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:14.663Z",
          "iopub.status.busy": "2020-06-11T17:27:14.654Z",
          "iopub.status.idle": "2020-06-11T17:27:14.675Z",
          "shell.execute_reply": "2020-06-11T17:27:14.683Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the train data set\n",
        "train_num = train_images.shape[0]\n",
        "train_random = np.arange(train_num)\n",
        "np.random.shuffle(train_random)\n",
        "train_images = train_images[train_random, :]\n",
        "train_labels = train_labels[train_random]\n",
        "train_labels = to_categorical(train_labels, num_classes=10)\n",
        "\n",
        "# shuffle the test data set\n",
        "test_num = test_images.shape[0]\n",
        "test_random = np.arange(test_num)\n",
        "np.random.shuffle(test_random)\n",
        "test_images = test_images[test_random,:]\n",
        "test_labels = test_labels[test_random]\n",
        "test_labels = to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "# split the validation data set and count the number of each data set\n",
        "valid_num = int(0.1*train_num)\n",
        "train_num = train_num-valid_num\n",
        "valid_images = train_images[:valid_num,:]\n",
        "valid_labels = train_labels[:valid_num,:]\n",
        "train_images = train_images[valid_num:,:]\n",
        "train_labels = train_labels[valid_num:,:]\n",
        "print(\"shape of train images:\", train_images.shape, \"\\tshape of train labels:\", train_labels.shape)\n",
        "print(\"shape of validation images:\", valid_images.shape, \"\\tshape of validation labels:\", valid_labels.shape)\n",
        "print(\"shape of test imagees:\", test_images.shape, \"\\tshape of test labels:\", test_labels.shape)\n",
        "\n",
        "# preprocess images and reshape image data\n",
        "train_images = train_images/255\n",
        "valid_images = valid_images/255\n",
        "test_images = test_images/255\n",
        "train_images = train_images.reshape(-1,1,28,28)\n",
        "valid_images = valid_images.reshape(-1,1,28,28)\n",
        "test_images = test_images.reshape(-1,1,28,28)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train images: (54000, 28, 28) \tshape of train labels: (54000, 10)\n",
            "shape of validation images: (6000, 28, 28) \tshape of validation labels: (6000, 10)\n",
            "shape of test imagees: (10000, 28, 28) \tshape of test labels: (10000, 10)\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:17.495Z",
          "iopub.status.busy": "2020-06-11T17:27:17.486Z",
          "iopub.status.idle": "2020-06-11T17:27:17.957Z",
          "shell.execute_reply": "2020-06-11T17:27:17.984Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method of evaluation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    calculate the total accuracy of the data set\n",
        "    :param y_pred: prediction of the model | shape (n,num_classes)\n",
        "    :param y_true: true labels of the data set | shape(n,num_classes)\n",
        "    :return acc: the total accuracy | shape(1,)\n",
        "    \"\"\"\n",
        "    return np.mean(np.equal(np.argmax(y_pred,axis=1), np.argmax(y_true,axis=1)))\n",
        "\n",
        "\n",
        "def categorical_precision(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    calculate the precision of each category\n",
        "    :param y_pred: prediction of the model | shape (n,num_classes)\n",
        "    :param y_true: true labels of the data set | shape(n,num_classes)\n",
        "    :return acc: the precision of each category | shape(1,cun_classes)\n",
        "    \"\"\"\n",
        "    pred = np.argmax(y_pred, axis=1)\n",
        "    pred = to_categorical(pred, num_classes=10)\n",
        "    # count the number of each category in the prediction of the model\n",
        "    pred_ca_num = np.sum(pred, axis=0)\n",
        "    true = np.transpose(y_true)\n",
        "    acc = np.dot(true, pred)\n",
        "    # count the true numer of each category in the prediction of the model\n",
        "    true_ca_num = []\n",
        "    for i in range(10):\n",
        "        true_ca_num.append(acc[i,i])\n",
        "    true_ca_num = np.array(true_ca_num)\n",
        "    return true_ca_num/pred_ca_num"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:21.940Z",
          "iopub.status.busy": "2020-06-11T17:27:21.930Z",
          "iopub.status.idle": "2020-06-11T17:27:21.954Z",
          "shell.execute_reply": "2020-06-11T17:27:21.962Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save model and load model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(path):\n",
        "    \"\"\"\n",
        "    save weights, nuerons, and gradients in three files and save other details of the model\n",
        "    :param path: the folder used to save model\n",
        "    \"\"\"\n",
        "    global lr, epoch, valid_loss, valid_acc, valid_ca_acc\n",
        "    details = {\"lr\":lr, \"epoch\":epoch, \"valid_loss\":valid_loss, \"valid_acc\":valid_acc, \"valid_ca_prec\":valid_ca_prec}\n",
        "    np.save(os.path.join(path,'weights.npy'), weights) \n",
        "    np.save(os.path.join(path,'nuerons.npy'), nuerons) \n",
        "    np.save(os.path.join(path,'gradients.npy'), gradients) \n",
        "    np.save(os.path.join(path,'details.npy'), details)\n",
        "    print(\"saving model to\", path, end=\"\\n\\n\")\n",
        "    \n",
        "    \n",
        "def load_model(path):\n",
        "    \"\"\"\n",
        "    load weights, nuerons, and gradients and other details of the saved model\n",
        "    :param path: the folder used to load model\n",
        "    :return: weights, nuerons, gradients, lr, epoch, valid_loss, valid_acc, valid_ca_prec\n",
        "    \"\"\"\n",
        "    weights = np.load(os.path.join(path,'weights.npy'), allow_pickle=True).item()\n",
        "    nuerons = np.load(os.path.join(path,'nuerons.npy'), allow_pickle=True).item()\n",
        "    gradients = np.load(os.path.join(path,'gradients.npy'), allow_pickle=True).item()\n",
        "    details = np.load(os.path.join(path,'details.npy'), allow_pickle=True).item()\n",
        "    return weights, nuerons, gradients, [details[\"lr\"], details[\"epoch\"], details[\"valid_loss\"], details[\"valid_acc\"], details[\"valid_ca_prec\"]]"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:23.092Z",
          "iopub.status.busy": "2020-06-11T17:27:23.083Z",
          "iopub.status.idle": "2020-06-11T17:27:23.105Z",
          "shell.execute_reply": "2020-06-11T17:27:23.113Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = {}\n",
        "nuerons={}\n",
        "gradients={}\n",
        "\n",
        "weights_scale = 1e-1\n",
        "filters = 32\n",
        "fc_units=64\n",
        "weights[\"K1\"] = weights_scale * np.random.randn(1, filters, 3, 3).astype(np.float64)\n",
        "weights[\"b1\"] = np.zeros(filters).astype(np.float64)\n",
        "weights[\"K2\"] = weights_scale * np.random.randn(filters, filters, 3, 3).astype(np.float64)\n",
        "weights[\"b2\"] = np.zeros(filters).astype(np.float64)\n",
        "weights[\"W3\"] = weights_scale * np.random.randn(800, fc_units).astype(np.float64)\n",
        "weights[\"b3\"] = np.zeros(fc_units).astype(np.float64)\n",
        "weights[\"W4\"] = weights_scale * np.random.randn(fc_units, 10).astype(np.float64)\n",
        "weights[\"b4\"] = np.zeros(10).astype(np.float64)\n",
        "\n",
        "\n",
        "\n",
        "v_weights = {}\n",
        "for itm in weights.keys():\n",
        "    v_weights[itm] = np.zeros_like(weights[itm])\n",
        "\n",
        "\n",
        "# define the total forward propagation\n",
        "def forward(X):\n",
        "    nuerons[\"conv1\"]=conv2d_fp(X.astype(np.float64), weights[\"K1\"],weights[\"b1\"])\n",
        "    nuerons[\"conv1_relu\"]=relu_fp(nuerons[\"conv1\"])\n",
        "    nuerons[\"maxp1\"]=maxpool_fp(nuerons[\"conv1_relu\"].astype(np.float64))\n",
        "\n",
        "    nuerons[\"conv2\"]=conv2d_fp(nuerons[\"maxp1\"], weights[\"K2\"], weights[\"b2\"])\n",
        "    nuerons[\"conv2_relu\"]=relu_fp(nuerons[\"conv2\"].astype(np.float64))\n",
        "    nuerons[\"maxp2\"]=maxpool_fp(nuerons[\"conv2_relu\"].astype(np.float64))\n",
        "    \n",
        "    \n",
        "    nuerons[\"flatten\"]=flatten_fp(nuerons[\"maxp2\"])\n",
        "\n",
        "    nuerons[\"fc2\"]=fc_fp(nuerons[\"flatten\"],weights[\"W3\"],weights[\"b3\"])\n",
        "    nuerons[\"fc2_relu\"]=relu_fp(nuerons[\"fc2\"])\n",
        "\n",
        "    nuerons[\"y\"]=fc_fp(nuerons[\"fc2_relu\"],weights[\"W4\"],weights[\"b4\"])\n",
        "\n",
        "    return nuerons[\"y\"]\n",
        "\n",
        "# define the total backward propagation\n",
        "def backward(X,y_true):\n",
        "    loss, dy = cross_entropy(nuerons[\"y\"],y_true)\n",
        "    gradients[\"W4\"], gradients[\"b4\"], gradients[\"fc2_relu\"] = fc_bp(dy, weights[\"W4\"], nuerons[\"fc2_relu\"])\n",
        "    gradients[\"fc2\"] = relu_bp(gradients[\"fc2_relu\"], nuerons[\"fc2\"])\n",
        "\n",
        "    gradients[\"W3\"], gradients[\"b3\"], gradients[\"flatten\"] = fc_bp(gradients[\"fc2\"], weights[\"W3\"], nuerons[\"flatten\"])\n",
        "        \n",
        "    gradients[\"maxp2\"] = flatten_bp(gradients[\"flatten\"], nuerons[\"maxp2\"])\n",
        "    gradients[\"conv2_relu\"] = maxpool_bp(gradients[\"maxp2\"].astype(np.float64), nuerons[\"conv2_relu\"].astype(np.float64))\n",
        "    gradients[\"conv2\"] = relu_bp(gradients[\"conv2_relu\"], nuerons[\"conv2\"])\n",
        "    gradients[\"K2\"], gradients[\"b2\"], gradients[\"maxp1\"] = conv2d_bp(gradients[\"conv2\"], weights[\"K2\"], nuerons[\"maxp1\"])\n",
        "    \n",
        "    gradients[\"conv1_relu\"] = maxpool_bp(gradients[\"maxp1\"].astype(np.float64), nuerons[\"conv1_relu\"].astype(np.float64))\n",
        "    gradients[\"conv1\"] = relu_bp(gradients[\"conv1_relu\"], nuerons[\"conv1\"])\n",
        "    gradients[\"K1\"], gradients[\"b1\"], _ = conv2d_bp(gradients[\"conv1\"], weights[\"K1\"], X)\n",
        "    return loss"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:49.227Z",
          "iopub.status.busy": "2020-06-11T17:27:49.220Z",
          "iopub.status.idle": "2020-06-11T17:27:49.241Z",
          "shell.execute_reply": "2020-06-11T17:27:49.248Z"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "epoch = 16\n",
        "batch_size = 32\n",
        "lr = 0.1\n",
        "it = 0\n",
        "\n",
        "# decrease the lr to lr*lr_drop_rate every epoch_step after the accuracy of validation data set isn't improve\n",
        "lr_drop_rate = 0.5\n",
        "epoch_step = 4\n",
        "\n",
        "train_step = train_num // batch_size\n",
        "valid_step = valid_num // batch_size\n",
        "\n",
        "# record the best accuracy of validation data set\n",
        "best_valid_acc = float(\"-inf\")\n",
        "# record the number of eoch after best_valid_acc isn't upgraded\n",
        "valid_epoch = 0\n",
        "\n",
        "# create a folder to save the best model\n",
        "save_path = time.strftime(\"%y%m%d%H%M%S\", time.localtime())\n",
        "os.makedirs(save_path)\n",
        "\n",
        "for i in range(epoch):\n",
        "    print(\"---------------------------------------------------\")\n",
        "    print(\"epoch:%03d\"%(i))\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for j in range(train_step):\n",
        "        train_pred = forward(train_images[j*batch_size:(j+1)*batch_size])\n",
        "        train_loss = backward(train_images[j*batch_size:(j+1)*batch_size], train_labels[j*batch_size:(j+1)*batch_size])\n",
        "        \n",
        "        sgd(weights, gradients, lr=lr)\n",
        "#         lr, it = sgd_md(weights, gradients, v_weights, lr, it)\n",
        "        \n",
        "        # validate on validation images every 1000 steps\n",
        "        if j%1000==0:\n",
        "            valid_pred = np.zeros((valid_num,10))\n",
        "            for k in range(valid_step):\n",
        "                valid_pred[k*batch_size:(k+1)*batch_size] = forward(valid_images[k*batch_size:(k+1)*batch_size])\n",
        "            if valid_step!=0:\n",
        "                k += 1\n",
        "            else:\n",
        "                k = 0\n",
        "            if valid_num-k*batch_size != 0:\n",
        "                valid_pred[k*batch_size:] = forward(valid_images[k*batch_size:])\n",
        "            valid_loss,_ = cross_entropy(valid_pred, valid_labels)\n",
        "            valid_acc = accuracy(valid_pred, valid_labels)\n",
        "            print(\"\\nstep:\",int(j))\n",
        "            print(\"validation loss:\", valid_loss)\n",
        "            print(\"validation accuracy:\", valid_acc)\n",
        "    \n",
        "    if train_step!=0:\n",
        "        j += 1\n",
        "    else:\n",
        "        j = 0\n",
        "    if train_num-j*batch_size != 0:\n",
        "        train_pred = forward(train_images[j*batch_size:])\n",
        "        train_loss = backward(train_images[j*batch_size:], train_labels[j*batch_size:])\n",
        "        \n",
        "        sgd(weights, gradients, lr=lr)\n",
        "#         lr, it = sgd_md(weights, gradients, v_weights, lr, it)\n",
        "    \n",
        "    print(\"*************\")\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(\"time: %.2f s\"%(epoch_time))\n",
        "    \n",
        "    # validate on validation images after each epoch\n",
        "    valid_pred = np.zeros((valid_num,10))\n",
        "    for k in range(valid_step):\n",
        "        valid_pred[k*batch_size:(k+1)*batch_size] = forward(valid_images[k*batch_size:(k+1)*batch_size])\n",
        "    if valid_step!=0:\n",
        "        k += 1\n",
        "    else:\n",
        "        k = 0\n",
        "    if valid_num-k*batch_size != 0:\n",
        "        valid_pred[k*batch_size:] = forward(valid_images[k*batch_size:])\n",
        "    valid_loss,_ = cross_entropy(valid_pred, valid_labels)\n",
        "    valid_acc = accuracy(valid_pred, valid_labels)\n",
        "    valid_ca_prec = categorical_precision(valid_pred, valid_labels)\n",
        "    if valid_acc>best_valid_acc:\n",
        "        print(\"validation accuracy is improved from %.6f to %.6f\"%(best_valid_acc, valid_acc))\n",
        "        save_model(save_path)\n",
        "        best_valid_acc = valid_acc\n",
        "        valid_epoch = 0\n",
        "    else:\n",
        "        valid_epoch += 1\n",
        "    \n",
        "    print(\"validation loss:\", valid_loss)\n",
        "    print(\"validation accuracy:\", valid_acc)\n",
        "    print(\"validation categorical precision:\\n\", valid_ca_prec)\n",
        "    \n",
        "    #upgrade the learning rate\n",
        "    if valid_epoch == epoch_step:\n",
        "        print(\"learing rate is decreased from %.6f to %.6f\"%(lr, lr*lr_drop_rate))\n",
        "        lr = lr*lr_drop_rate\n",
        "        valid_epoch = 0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "epoch:000\n",
            "\n",
            "step: 0\n",
            "validation loss: 2.2568019377669897\n",
            "validation accuracy: 0.1155\n",
            "\n",
            "step: 1000\n",
            "validation loss: 0.12366508582116018\n",
            "validation accuracy: 0.9595\n",
            "*************\n",
            "time: 26414.94 s\n",
            "validation accuracy is improved from -inf to 0.962167\n",
            "saving model to 200612115604\n",
            "\n",
            "validation loss: 0.1213021075521217\n",
            "validation accuracy: 0.9621666666666666\n",
            "validation categorical precision:\n",
            " [0.9724919  0.9735294  0.9622302  0.9932773  0.84965515 0.9685315\n",
            " 0.9915254  0.9827316  0.96750903 0.987315  ]\n",
            "---------------------------------------------------\n",
            "epoch:001\n",
            "\n",
            "step: 0\n",
            "validation loss: 0.08343361118115951\n",
            "validation accuracy: 0.9731666666666666\n",
            "\n",
            "step: 1000\n",
            "validation loss: 0.07379857277010596\n",
            "validation accuracy: 0.9761666666666666\n",
            "*************\n",
            "time: 27339.57 s\n",
            "validation accuracy is improved from 0.962167 to 0.979167\n",
            "saving model to 200612115604\n",
            "\n",
            "validation loss: 0.06783454616135676\n",
            "validation accuracy: 0.9791666666666666\n",
            "validation categorical precision:\n",
            " [0.98042417 0.98511904 0.9783394  0.99335545 0.94753087 0.975265\n",
            " 0.9899329  0.98447204 0.9801802  0.97818184]\n",
            "---------------------------------------------------\n",
            "epoch:002\n",
            "\n",
            "step: 0\n",
            "validation loss: 0.061205708987731804\n",
            "validation accuracy: 0.9801666666666666\n",
            "\n",
            "step: 1000\n",
            "validation loss: 0.06687366446582156\n",
            "validation accuracy: 0.9801666666666666\n",
            "*************\n",
            "time: 27262.23 s\n",
            "validation accuracy is improved from 0.979167 to 0.984000\n",
            "saving model to 200612115604\n",
            "\n",
            "validation loss: 0.05151178860318108\n",
            "validation accuracy: 0.984\n",
            "validation categorical precision:\n",
            " [0.99174917 0.98656714 0.9767442  0.98844886 0.9918567  0.9753521\n",
            " 0.99161077 0.9875     0.9737762  0.97363794]\n",
            "---------------------------------------------------\n",
            "epoch:003\n",
            "\n",
            "step: 0\n",
            "validation loss: 0.05165033783694281\n",
            "validation accuracy: 0.984\n",
            "\n",
            "step: 1000\n",
            "validation loss: 0.06957164766637867\n",
            "validation accuracy: 0.979\n",
            "*************\n",
            "time: 27359.90 s\n",
            "validation accuracy is improved from 0.984000 to 0.985333\n",
            "saving model to 200612115604\n",
            "\n",
            "validation loss: 0.05385738160049802\n",
            "validation accuracy: 0.9853333333333333\n",
            "validation categorical precision:\n",
            " [0.99502486 0.99544764 0.97359157 0.9917355  0.9934534  0.978836\n",
            " 0.99329984 0.9920886  0.9724613  0.96360487]\n",
            "---------------------------------------------------\n",
            "epoch:004\n",
            "\n",
            "step: 0\n",
            "validation loss: 0.053791046525984994\n",
            "validation accuracy: 0.9851666666666666\n",
            "\n",
            "step: 1000\n",
            "validation loss: 0.06994803539544713\n",
            "validation accuracy: 0.9785\n",
            "*************\n",
            "time: 26620.97 s\n",
            "validation loss: 0.059218484920472436\n",
            "validation accuracy: 0.9843333333333333\n",
            "validation categorical precision:\n",
            " [0.99667776 0.99542683 0.97183096 0.9884679  0.9967051  0.980531\n",
            " 0.9933222  0.9904913  0.9609508  0.9652778 ]\n",
            "---------------------------------------------------\n",
            "epoch:005\n",
            "\n",
            "step: 0\n",
            "validation loss: 0.05918459316869797\n",
            "validation accuracy: 0.9843333333333333\n",
            "\n",
            "step: 1000\n",
            "validation loss: 0.08303978829661385\n",
            "validation accuracy: 0.9766666666666667\n",
            "*************\n",
            "time: 26545.24 s\n",
            "validation loss: 0.06170228921759511\n",
            "validation accuracy: 0.9831666666666666\n",
            "validation categorical precision:\n",
            " [0.99832493 0.9969419  0.9717813  0.98039216 0.9918033  0.978836\n",
            " 0.9899666  0.9905363  0.9609508  0.9685315 ]\n",
            "---------------------------------------------------\n",
            "epoch:006\n",
            "\n",
            "step: 0\n",
            "validation loss: 0.06307375576272257\n",
            "validation accuracy: 0.9823333333333333\n",
            "\n",
            "step: 1000\n",
            "validation loss: 0.09389991749999695\n",
            "validation accuracy: 0.9746666666666667\n",
            "*************\n",
            "time: 26609.63 s\n",
            "validation loss: 0.06391781625174643\n",
            "validation accuracy: 0.984\n",
            "validation categorical precision:\n",
            " [0.9966555  0.9969325  0.97359157 0.9820555  0.9934534  0.9770723\n",
            " 0.99329984 0.99210113 0.9609508  0.9702797 ]\n",
            "---------------------------------------------------\n",
            "epoch:007\n",
            "\n",
            "step: 0\n",
            "validation loss: 0.06484471450927089\n",
            "validation accuracy: 0.984\n",
            "\n",
            "step: 1000\n",
            "validation loss: 0.08484887652815373\n",
            "validation accuracy: 0.9778333333333333\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-24-95b2948f59e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtrain_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-23-4a3f539d3803>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mnuerons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"conv1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconv2d_fp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"K1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mnuerons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"conv1_relu\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrelu_fp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnuerons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"conv1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mnuerons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"maxp1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxpool_fp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnuerons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"conv1_relu\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mnuerons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"conv2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconv2d_fp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnuerons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"maxp1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"K2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-6-ba8e116be6f7>\u001b[0m in \u001b[0;36mmaxpool_fp\u001b[1;34m(z, pool_size, pool_stride, padding)\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                     \u001b[0mz_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                     \u001b[0mz_po\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_part\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mz_po\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2666\u001b[0m     \"\"\"\n\u001b[0;32m   2667\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[1;32m-> 2668\u001b[1;33m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-06-11T17:27:50.552Z",
          "iopub.status.busy": "2020-06-11T17:27:50.545Z",
          "iopub.status.idle": "2020-06-11T17:27:34.077Z",
          "shell.execute_reply": "2020-06-11T17:27:34.403Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# test directly\n",
        "test_pred = forward(test_images)\n",
        "test_pred = softmax(test_pred)\n",
        "test_loss,_ = cross_entropy(test_pred, test_labels)\n",
        "test_acc = accuracy(test_pred, test_labels)\n",
        "test_ca_prec = categorical_precision(test_pred, test_labels)\n",
        "print(\"test loss:\", test_loss)\n",
        "print(\"test accuracy:\", test_acc)\n",
        "print(\"test categorical precision:\\n\", test_ca_prec)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 1.4754496099655094\n",
            "test accuracy: 0.9879\n",
            "test categorical precision:\n",
            " [0.9898374  0.9973262  0.9836381  0.98710316 0.99285716 0.9789123\n",
            " 0.9926393  0.9882927  0.982582   0.9842209 ]\n"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# test from the model saved\n",
        "load_path = \"200612115604\"\n",
        "weights, nuerons, gradients, details = load_model(load_path)\n",
        "\n",
        "test_pred = forward(test_images)\n",
        "test_pred = softmax(test_pred)\n",
        "test_loss,_ = cross_entropy(test_pred, test_labels)\n",
        "test_acc = accuracy(test_pred, test_labels)\n",
        "test_ca_prec = categorical_precision(test_pred, test_labels)\n",
        "print(\"test loss:\", test_loss)\n",
        "print(\"test accuracy:\", test_acc)\n",
        "print(\"test categorical precision:\\n\", test_ca_prec)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 1.479828412964322\n",
            "test accuracy: 0.9856\n",
            "test categorical precision:\n",
            " [0.9918117  0.9946524  0.9826087  0.98035365 0.99486655 0.9853604\n",
            " 0.98957247 0.99404764 0.964965   0.97745097]\n"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application of the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "All functions defined in this cell have already been defined above to train the model.\n",
        "Redefinition here simply serves for the application of the saved model.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import struct\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# the file of test images\n",
        "test_images_idx3_ubyte_file = 't10k-images.idx3-ubyte'\n",
        "# load data\n",
        "def decode_idx3_ubyte(idx3_ubyte_file):\n",
        "    bin_data = open(idx3_ubyte_file, 'rb').read()\n",
        "    # decode file header information\n",
        "    offset = 0\n",
        "    fmt_header = '>iiii'\n",
        "    magic_number, num_images, num_rows, num_cols = struct.unpack_from(fmt_header, bin_data, offset)\n",
        "    print ('magic number:%d, images number: %d, image size: %d*%d' % (magic_number, num_images, num_rows, num_cols))\n",
        "    # decode data set\n",
        "    image_size = num_rows * num_cols\n",
        "    offset += struct.calcsize(fmt_header)\n",
        "    fmt_image = '>' + str(image_size) + 'B'\n",
        "    images = np.empty((num_images, num_rows, num_cols))\n",
        "    for i in range(num_images):\n",
        "        if (i + 1) % 10000 == 0:\n",
        "            print ('have decoded %d' % (i + 1) + 'images')\n",
        "        images[i] = np.array(struct.unpack_from(fmt_image, bin_data, offset)).reshape((num_rows, num_cols))\n",
        "        offset += struct.calcsize(fmt_image)\n",
        "    return images\n",
        "\n",
        "def load_test_images(idx_ubyte_file=test_images_idx3_ubyte_file):\n",
        "    return decode_idx3_ubyte(idx_ubyte_file)\n",
        "\n",
        "test_images = load_test_images()\n",
        "test_images = test_images/255\n",
        "test_images = test_images.reshape(-1,1,28,28)\n",
        "\n",
        "# load model\n",
        "def load_model(path):\n",
        "    weights = np.load(os.path.join(path,'weights.npy'), allow_pickle=True).item()\n",
        "    nuerons = np.load(os.path.join(path,'nuerons.npy'), allow_pickle=True).item()\n",
        "    gradients = np.load(os.path.join(path,'gradients.npy'), allow_pickle=True).item()\n",
        "    details = np.load(os.path.join(path,'details.npy'), allow_pickle=True).item()\n",
        "    return weights, nuerons, gradients, [details[\"lr\"], details[\"epoch\"], details[\"valid_loss\"], details[\"valid_acc\"], details[\"valid_ca_prec\"]]  \n",
        "\n",
        "# define the function used in the model\n",
        "def fc_fp(z, W, b):\n",
        "    z_fc = np.dot(z, W) + b\n",
        "    return z_fc\n",
        "\n",
        "def conv2d_fp(z, K, b, padding=(0,0)):\n",
        "    p1 = padding[0]\n",
        "    p2 = padding[1]\n",
        "    z_p = np.lib.pad(z, ((0,0),(0,0),(p1,p1),(p2,p2)), mode='constant', constant_values=0)\n",
        "    n, c1, h1, w1 = z_p.shape\n",
        "    c1, c2, k1, k2 = K.shape\n",
        "    h2 = h1-k1+1\n",
        "    w2 = w1-k2+1\n",
        "    z_conv = np.zeros((n, c2, h2, w2))\n",
        "    for num in range(n):\n",
        "        for c in range(c2):\n",
        "            for h in range(h2):\n",
        "                for w in range(w2):\n",
        "                    z_part = z_p[num, :, h:h+k1, w:w+k2]\n",
        "                    K_part = K[:,c]\n",
        "                    z_conv[num, c, h, w] = np.sum(z_part * K_part) + b[c]\n",
        "    return z_conv\n",
        "\n",
        "def maxpool_fp(z, pool_size=(2,2), pool_stride=(2,2), padding=(0,0)):\n",
        "    n, c1, h1, w1 = z.shape\n",
        "    k1 = pool_size[0]\n",
        "    k2 = pool_size[1]\n",
        "    s1 = pool_stride[0]\n",
        "    s2 = pool_stride[1]\n",
        "    p1 = padding[0]\n",
        "    p2 = padding[1]\n",
        "    h2 = (h1 + p1*2 - k1)//s1 + 1\n",
        "    w2 = (w1 + p2*2 - k2)//s2 + 1\n",
        "    z_p = np.lib.pad(z, ((0,0), (0,0), (p1, p1), (p2, p2)), 'constant', constant_values=0)\n",
        "    z_po = np.zeros((n,c1,h2,w2))\n",
        "    for num in range(n):\n",
        "        for c in range(c1):\n",
        "            for h in range(h2):\n",
        "                for w in range(w2):\n",
        "                    z_part = z_p[num, c, h*s1:h*s1+k1, w*s2:w*s2+k2]\n",
        "                    z_po[num,c,h,w] = np.max(z_part)\n",
        "    return z_po\n",
        "\n",
        "def flatten_fp(z):\n",
        "    \"\"\" change the dimention of data: 2D to 1D\n",
        "    :param z: output of previous layer | shape(n,c1,h1,w1)\n",
        "    :return z_flt: output after flattening | shape(n,l1)\n",
        "    \"\"\"\n",
        "    n = z.shape[0]\n",
        "    z_flt = np.reshape(z, (n,-1))\n",
        "    return z_flt\n",
        "\n",
        "def relu_fp(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def softmax(y):\n",
        "    return (np.exp(y).T/np.sum(np.exp(y), axis=-1)).T\n",
        "\n",
        "# define the total forward propagation\n",
        "def forward(X):\n",
        "    nuerons[\"conv1\"]=conv2d_fp(X.astype(np.float64), weights[\"K1\"],weights[\"b1\"])\n",
        "    nuerons[\"conv1_relu\"]=relu_fp(nuerons[\"conv1\"])\n",
        "    nuerons[\"maxp1\"]=maxpool_fp(nuerons[\"conv1_relu\"].astype(np.float64))\n",
        "    nuerons[\"conv2\"]=conv2d_fp(nuerons[\"maxp1\"], weights[\"K2\"], weights[\"b2\"])\n",
        "    nuerons[\"conv2_relu\"]=relu_fp(nuerons[\"conv2\"].astype(np.float64))\n",
        "    nuerons[\"maxp2\"]=maxpool_fp(nuerons[\"conv2_relu\"].astype(np.float64))\n",
        "    nuerons[\"flatten\"]=flatten_fp(nuerons[\"maxp2\"])\n",
        "    nuerons[\"fc2\"]=fc_fp(nuerons[\"flatten\"],weights[\"W3\"],weights[\"b3\"])\n",
        "    nuerons[\"fc2_relu\"]=relu_fp(nuerons[\"fc2\"])\n",
        "    nuerons[\"y\"]=fc_fp(nuerons[\"fc2_relu\"],weights[\"W4\"],weights[\"b4\"])\n",
        "    return nuerons[\"y\"]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "magic number:2051, images number: 10000, image size: 28*28\n",
            "have decoded 10000images\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.status.busy": "2020-06-15T04:01:34.766Z",
          "iopub.execute_input": "2020-06-15T04:01:34.776Z",
          "iopub.status.idle": "2020-06-15T04:01:35.563Z",
          "shell.execute_reply": "2020-06-15T04:01:35.588Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_path = \"200612115604\"\n",
        "weights, nuerons, gradients, details = load_model(load_path)\n",
        "num = np.random.randint(0, test_images.shape[0], 1)\n",
        "\n",
        "# application: test a picture\n",
        "test_img = test_images[num].astype(np.float64)\n",
        "test_img = test_img.reshape(1,1,28,28)\n",
        "\n",
        "test_img_pred = softmax(forward(test_img))\n",
        "test_img_pred = np.argmax(test_img_pred, axis=1)\n",
        "\n",
        "plt.imshow(test_img[0][0], cmap='gray')\n",
        "print(\"\\nPrediction of this picture is:\", test_img_pred[0])\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction of this picture is: 9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": [
              "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\n"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.status.busy": "2020-06-15T04:02:07.977Z",
          "iopub.execute_input": "2020-06-15T04:02:07.987Z",
          "iopub.status.idle": "2020-06-15T04:02:08.318Z",
          "shell.execute_reply": "2020-06-15T04:02:08.337Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.23.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}